{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dJsRS12uN4U",
    "outputId": "f79db818-053b-4965-fec1-14d1d7cc3a79"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import itertools\n",
    "import functools\n",
    "import operator\n",
    "import json\n",
    "import psycopg2\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'test/DG_for_colab'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/DG_results'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/inclusions_with_patient'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/inclusions_without_patient'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/exclusions_with_patient'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/exclusions_without_patient'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/frequency_tables'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/frequency_tables_expand_by_f_and_num_criteria'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/frequency_tables_expand_sorted_by_f'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'test/condition_with_no_trials_and_patterns'\n"
     ]
    }
   ],
   "source": [
    "# geneate directories\n",
    "parent_dir = \"test/\"\n",
    "dirs = [\"DG_for_colab\", \"DG_results\", \n",
    "         \"inclusions_with_patient\", \"inclusions_without_patient\", \n",
    "         \"exclusions_with_patient\", \"exclusions_without_patient\",\n",
    "        \"frequency_tables\", \"frequency_tables_expand_by_f_and_num_criteria\",\n",
    "        \"frequency_tables_expand_sorted_by_f\", \"condition_with_no_trials_and_patterns\" ]\n",
    "for dir in dirs:\n",
    "    try:\n",
    "        os.mkdir(parent_dir + dir)\n",
    "    except OSError as error:  \n",
    "        print(error)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Pvdd2Ua_uPz1"
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"cdm_5.2.2_SynPUF5pct\",\n",
    "    user=\"postgres\",\n",
    "    password=\"123456\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wT3hQRFeuU9l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116352"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_total_numPatients_in_database():\n",
    "    sql = \"select count(distinct person_id) from person\"\n",
    "    cur.execute(sql)\n",
    "    result = cur.fetchone()\n",
    "    if result != None:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return 0\n",
    "compute_total_numPatients_in_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZNfBAyGJuV7z"
   },
   "outputs": [],
   "source": [
    "## Setting\n",
    "TOP_K_CRITERIA = 25\n",
    "UPPER_BOUND = 5 #The upper bound of the number of criteria in a pattern\n",
    "TOP_M_PATTERNS = 20\n",
    "condition_ids = [313217]#, 317576, 314658, 314665] #Conditions that will be analyzed\n",
    "condition_names = {313217:'Atrial fibrillation', \n",
    "                   317576:'Coronary arteriosclerosis',\n",
    "                   314658:'Cardiomegaly',\n",
    "                   314665:'Atrial flutter'}\n",
    "general_criteria = [4274025, 4322976] #Some general criteria that will be excluded from the descendant_concept list.\n",
    "TOTAL_NUMPATIENTS = compute_total_numPatients_in_database() #We may need to round it\n",
    "color_dic = {1:'lightcoral', 2:'moccasin', 3:'palegreen', 4:'lightskyblue', 5:'violet'} #color of the nodes in the directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "R4CfU6HHuXiP"
   },
   "outputs": [],
   "source": [
    "dic_domain_table = {\"Condition\": (\"condition_occurrence\", \"condition_concept_id\"), \n",
    "       \"Device\": (\"device_exposure\", \"device_concept_id\"), \n",
    "       \"Drug\": (\"drug_exposure\", \"drug_concept_id\"),\n",
    "      \"Observation\": (\"observation\", \"observation_concept_id\"),\n",
    "      \"Measurement\": (\"measurement\",\"measurement_concept_id\"),\n",
    "      \"Procedure\": (\"procedure_occurrence\", \"procedure_concept_id\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tZOt2hi1uZLX"
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(''+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mw2AGyBcua29"
   },
   "outputs": [],
   "source": [
    "def concept_set_sql_formulate(concept_id, inc):\n",
    "    #This function is to formulate the sql query to get all the peron_ids corresponding to a concept and its \n",
    "    #descendant concepts. The person in the list has at least one record of the concept or its descndant concepts.\n",
    "    #It return the sql query and the concept_name of the concept.\n",
    "    \n",
    "    #Retrieve all the descendant_concept_ids of a concept\n",
    "    sql1 = \"SELECT descendant_concept_id \\\n",
    "    FROM concept_ancestor \\\n",
    "    where ancestor_concept_id = \"+str(concept_id)+\";\"\n",
    "    cur.execute(sql1)\n",
    "    desc_ids = cur.fetchall()\n",
    "    desc_ids = tuple([item[0] for item in desc_ids])\n",
    "    \n",
    "    \n",
    "    print(\"number of descendants:\", len(desc_ids))\n",
    "    if len(desc_ids)>10000:\n",
    "        print(concept_id, \"number of descendant_concepts is larger than 10000\")\n",
    "    sql2 = \"\"\n",
    "    dic_domain_concept = {\"Condition\":[], \"Device\":[],\"Drug\":[], \"Observation\":[], \"Measurement\":[],\"Procedure\":[]}\n",
    "    concept_name = \"\"\n",
    "    for desc_id in desc_ids:\n",
    "        #Get the domain of the descendant_concepts, and categorize the descendant_cocnepts with their domain type.\n",
    "        cur.execute(\n",
    "                       \"SELECT domain_id, concept_name FROM concept WHERE concept_id = \"+str(desc_id)+\";\"\n",
    "                        )\n",
    "        result = cur.fetchone()\n",
    "        if result != None:\n",
    "            #If there is record of this descendant_concept in the database:\n",
    "            domain = result[0]\n",
    "            dic_domain_concept[domain].append(desc_id)\n",
    "            if desc_id == concept_id:\n",
    "                concept_name = result[1]\n",
    "                print(\"concept_name:\",concept_name)\n",
    "    \n",
    "    flag_domain_voc_contained = False\n",
    "    for domain in dic_domain_concept.keys():\n",
    "        table_name = dic_domain_table[domain][0]\n",
    "        concept_column = dic_domain_table[domain][1]\n",
    "        if dic_domain_concept[domain]!=[]:\n",
    "            #If there exists descendant_concepts with the specific domain type, we query the table of this domain\n",
    "            #and get all the person_ids with one of these descendant_concepts. \n",
    "            flag_domain_voc_contained = True\n",
    "            sql2 += \"(SELECT DISTINCT person_id FROM \"+ table_name +\\\n",
    "                        \" WHERE \"+concept_column +\" in (\"\n",
    "            id_list = functools.reduce(lambda x, y:str(x) +\", \"+str(y),dic_domain_concept[domain])\n",
    "            sql2 += str(id_list)\n",
    "            sql2 += \")) UNION \"\n",
    "\n",
    "   \n",
    "    if flag_domain_voc_contained:\n",
    "        sql2 = sql2[:-6]\n",
    "           \n",
    "           \n",
    "    if inc == False:\n",
    "        #If the concept is an exclusion:\n",
    "        sql2 = \"SELECT DISTINCT person_id FROM person EXCEPT (\"+sql2+\")\"       \n",
    "                \n",
    "    return sql2, concept_name\n",
    "\n",
    "def search_database_concept_set_descendant(concept_id, inc):\n",
    "    #This function is to retrieve all the set of person_ids related to a concept or its descendant cocnepts. \n",
    "    #The person in the list has at least one record of the concept or its descndant concepts.\n",
    "    sql, concept_name = concept_set_sql_formulate(concept_id, inc)\n",
    "    if sql != \"\":\n",
    "        cur.execute(sql)\n",
    "        all_person_ids = cur.fetchall()\n",
    "        all_person_ids = set(person_id[0] for person_id in all_person_ids)\n",
    "    else:\n",
    "        all_person_ids = set()\n",
    "    return all_person_ids, concept_name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-NO-NbA1ucP9"
   },
   "outputs": [],
   "source": [
    "def name_combination(row):\n",
    "    ids = row['pattern']\n",
    "    names = []\n",
    "    for id in ids:\n",
    "        if id>0:\n",
    "            name = dic_inc_concept_names[id]\n",
    "            names.append(name)\n",
    "        else:\n",
    "            name = dic_exc_concept_names[abs(id)]\n",
    "            names.append(\"-\"+name)\n",
    "    return names\n",
    "    \n",
    "\n",
    "def flag_and_negate_exc(row):\n",
    "    #This function is to flag whether the criterion is in the Top K inclusion or exclusion criteria list. \n",
    "    #It also flags the exclusion criterion.\n",
    "    concept_id = row['criteria_concept_id']\n",
    "    if int(concept_id in inclusions):\n",
    "        row['flag']=1\n",
    "    elif int(concept_id in exclusions):\n",
    "        row['flag']=2\n",
    "        #change the exclusion_criteria_id from positive to negative so as to distinguish exclusion and inclusion criteria. \n",
    "        row['criteria_concept_id'] = -row['criteria_concept_id']\n",
    "    else:\n",
    "        row['flag']=0\n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SYEoGqBvud-E"
   },
   "outputs": [],
   "source": [
    "def traverse_combinations(row):\n",
    "    #This function is to traverse all the combinations of the criteria in a pattern, and assign them the frequency\n",
    "    #with value equal to the one of the pattern.\n",
    "    def position_id_conversion(comb):\n",
    "        return tuple(map(lambda x: pattern[x], comb))\n",
    "    \n",
    "    pattern = eval(row[\"pattern\"])\n",
    "    num_criteria = len(pattern)\n",
    "    id_combs = []\n",
    "    upper_bound = min(UPPER_BOUND, num_criteria) #traverse the combinations of variables, the number of which is no more than UPPER_BOUND\n",
    "    for k in np.arange(1, upper_bound+1):\n",
    "        combinations = list(itertools.combinations(range(num_criteria),k))\n",
    "        id_combs_k = map(position_id_conversion, combinations)\n",
    "        id_combs += id_combs_k\n",
    "    num_combs = len(id_combs)\n",
    "    frequency = num_combs * [row[1]]\n",
    "    return list(zip(id_combs, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Z31SeIFCugIj"
   },
   "outputs": [],
   "source": [
    "def compute_count(concept_ids):\n",
    "    #Compute patient count of patterns and sub-patterns\n",
    "    inter_persons = criteria[abs(concept_ids[0])]\n",
    "    if len(concept_ids)>1:\n",
    "        for id in concept_ids[1:]:\n",
    "            inter_persons = inter_persons.intersection(criteria[abs(id)])\n",
    "    count = len(inter_persons)\n",
    "    return count\n",
    "        \n",
    "def add_sub_pattern_with_largest_reduction_rate(pattern): \n",
    "    #Draw the directed graph to represent the network topology of patterns and sub-patterns with the largest reduction rate.\n",
    "    if len(pattern) > 1:\n",
    "        #print(pattern)\n",
    "        sub_patterns = list(itertools.combinations(pattern,len(pattern)-1))\n",
    "        row = []\n",
    "        count = 0 \n",
    "        i = 0\n",
    "        #print(\"length sub patterns\", len(sub_patterns))\n",
    "        pattern_with_largest_reduction_rate = ()\n",
    "        while True:\n",
    "            sub_pattern = sub_patterns[i]\n",
    "            row_temp = freq_table.loc[freq_table['pattern']==str(sub_pattern)]\n",
    "            count_temp = compute_count(sub_pattern)\n",
    "            if i == 1:\n",
    "                row = row_temp\n",
    "                count = count_temp\n",
    "                pattern_with_largest_reduction_rate = sub_pattern\n",
    "            else:\n",
    "                if count_temp>count:\n",
    "                    row = row_temp\n",
    "                    count = count_temp\n",
    "                    pattern_with_largest_reduction_rate = sub_pattern\n",
    "\n",
    "            i = i + 1\n",
    "            if(i >= len(list(sub_patterns))):\n",
    "                break\n",
    "       # if str(pattern_with_largest_reduction_rate) in top_patterns:\n",
    "       #     color = 'red'\n",
    "       # else:\n",
    "       #     color = 'blue'\n",
    "        num_criteria = len(pattern_with_largest_reduction_rate)\n",
    "        DG.add_nodes_from([(pattern_with_largest_reduction_rate,\n",
    "                           {\"names\": row['pattern_concept_names'].values[0], \n",
    "                            \"frequency\": row['frequency'].values[0], \n",
    "                            \"relative frequency\": row['relative_frequency'].values[0],\n",
    "                            \"count\": count,\n",
    "                           \"color\": color_dic[num_criteria]})])\n",
    "        count_pattern = DG.nodes[pattern]['count']\n",
    "        reduction_rate = round((count - count_pattern)/count,5)\n",
    "        reduction_rate = \"{:.2%}\".format(reduction_rate)\n",
    "        #print(pattern, pattern_with_largest_reduction_rate, reduction_rate)\n",
    "        DG.add_weighted_edges_from([(pattern_with_largest_reduction_rate, pattern, reduction_rate)])\n",
    "        add_sub_pattern_with_largest_reduction_rate(pattern_with_largest_reduction_rate)\n",
    "\n",
    "            \n",
    "            \n",
    "def add_sub_pattern(pattern): \n",
    "    #Draw the directed graph to represent the network topology of patterns and all sub-patterns.\n",
    "    row = freq_table.loc[freq_table['pattern']==str(pattern)]\n",
    "    count = compute_count(pattern)\n",
    "    #if str(pattern) in top_patterns:\n",
    "    #    color = 'red'\n",
    "    #else:\n",
    "    #    color = 'blue'\n",
    "    num_criteria = len(pattern)\n",
    "    DG.add_nodes_from([(pattern,\n",
    "                           {\"names\": row['pattern_concept_names'].values[0], \n",
    "                            \"frequency\": row['frequency'].values[0], \n",
    "                            \"relative frequency\": row['relative_frequency'].values[0],\n",
    "                            \"count\": count,\n",
    "                           \"color\": color_dic[num_criteria]})])\n",
    "    if len(pattern) != 1:\n",
    "        for sub_pattern in itertools.combinations(pattern,len(pattern)-1):\n",
    "            add_sub_pattern(sub_pattern)\n",
    "            count_pattern = DG.nodes[pattern]['count']\n",
    "            count_sub_pattern = DG.nodes[sub_pattern]['count']\n",
    "            reduction_rate = round((count_sub_pattern - count_pattern)/count_sub_pattern,5)\n",
    "            print(pattern, sub_pattern, reduction_rate)\n",
    "            DG.add_weighted_edges_from([(sub_pattern, pattern, reduction_rate)])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KpZAXfJ3uiCU"
   },
   "outputs": [],
   "source": [
    "def get_table_with_numPatients_for_each_concept(domain):\n",
    "    #Get the number of patients for each concept with domain, and create the table with field \"concept_name\", \n",
    "    #\"concept_id\" and \"number of patients\".\n",
    "    table_name = dic_domain_table[domain][0]\n",
    "    concept_id_column = dic_domain_table[domain][1]\n",
    "    sql = \"select p.concept_name, p.concept_id, count(p.person_id) as num_patients \\\n",
    "    from \\\n",
    "    (select distinct concept_name, concept_id, person_id \\\n",
    "    FROM \"+ table_name +\" JOIN concept \\\n",
    "    on concept_id = \"+concept_id_column +\" ) as p \\\n",
    "    group by p.concept_id, p.concept_name \\\n",
    "    order by count(p.person_id) desc;\"\n",
    "    cur.execute(sql)\n",
    "    table_content = cur.fetchall()\n",
    "    table_concept_numPatients = pd.DataFrame().from_dict(table_content)\n",
    "    table_concept_numPatients.columns = [\"concept_name\", \"concept_id\", \"num_patients\"]\n",
    "    return table_concept_numPatients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pJGs2wD3ukbZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Get the number of patients for each condition\\ncondition_numPatients = get_table_with_numPatients_for_each_concept(\"Condition\")\\ncondition_numPatients.to_csv(\"test/condition_numPatients.csv\", index=False, sep=\",\")\\n\\n#condition_numPatient = pd.read_csv(\"test/condition_numPatients.csv\",sep=\",\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Get the number of patients for each condition\n",
    "condition_numPatients = get_table_with_numPatients_for_each_concept(\"Condition\")\n",
    "condition_numPatients.to_csv(\"test/condition_numPatients.csv\", index=False, sep=\",\")\n",
    "\n",
    "#condition_numPatient = pd.read_csv(\"test/condition_numPatients.csv\",sep=\",\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TRyVnYU1unKn"
   },
   "outputs": [],
   "source": [
    "#Get the top K inclusion criteria and exclusion criteria. \n",
    "#If the number of criteria related to a condition is less than K, we keep all its criteria.\n",
    "#If a criterion appears in both the top K inclusion and exclusion criteria, the exclusion criterion is removed. \n",
    "ctkb_all_disease_top_inclusion_criteria_50 = pd.read_csv(\"ctkb_all_disease_top_inclusion_criteria_50_1.csv\",\n",
    "                                                         sep=\",\",header=None)\n",
    "ctkb_all_disease_top_exclusion_criteria_50 = pd.read_csv(\"ctkb_all_disease_top_exclusion_criteria_50_1.csv\",\n",
    "                                                         sep=\",\",header=None)\n",
    "inc_top = {}\n",
    "exc_top = {}\n",
    "for i in range(len(ctkb_all_disease_top_inclusion_criteria_50)):\n",
    "  inc = list(ctkb_all_disease_top_inclusion_criteria_50.iloc[i, 1:TOP_K_CRITERIA+1])\n",
    "  inc = [int(item) for item in inc if ~np.isnan(item)]\n",
    "\n",
    "  exc = list(ctkb_all_disease_top_exclusion_criteria_50.iloc[i, 1:TOP_K_CRITERIA+1])\n",
    "  exc = [int(item) for item in exc if ~np.isnan(item)]\n",
    "\n",
    "  inc_top[ctkb_all_disease_top_inclusion_criteria_50.iloc[i, 0]] = inc\n",
    "\n",
    "  inter_set = set(inc).intersection(set(exc))\n",
    "    \n",
    "  exc_top[ctkb_all_disease_top_exclusion_criteria_50.iloc[i, 0]] = list(set(exc)-inter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PY1ELarfupmw"
   },
   "outputs": [],
   "source": [
    "#Preprocess the ctkb_all_criteria table\n",
    "ctkb_all_criteria = pd.read_csv(\"ctkb_all_criteria.csv\" )\n",
    "ctkb_all_criteria[\"Count\"] = 1\n",
    "ctkb_all_criteria_dropped = ctkb_all_criteria.drop_duplicates()\n",
    "ctkb_all_criteria_dropped = ctkb_all_criteria_dropped[ctkb_all_criteria_dropped.criteria_concept_id!=\"unmapped\"]\n",
    "ctkb_all_criteria_dropped.criteria_concept_id = pd.to_numeric(ctkb_all_criteria_dropped.criteria_concept_id)\n",
    "#ctkb_all_criteria_dropped.head()\n",
    "\n",
    "#Preprocess the ctkb_all_trials table\n",
    "ctkb_all_trials = pd.read_csv(\"ctkb_all_trials.csv\" )\n",
    "ctkb_all_trials = ctkb_all_trials[[\"nctid\",\"condition_concept_id\"]]\n",
    "ctkb_all_trials = ctkb_all_trials.drop_duplicates() \n",
    "#ctkb_all_trials.sort_values(by=\"nctid\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MbPFMuXqutQ2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef jsonKeys2int(x):\\n    if isinstance(x, dict):\\n            return {int(k):v for k,v in x.items()}\\n    return x\\n\\nwith open(\"test/num_trials_per_condition.txt\", \\'r\\') as reader:\\n    dic=reader.read()\\ndic_num_trials = json.loads(dic,object_hook=jsonKeys2int)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the number of trials for each condition\n",
    "dic_num_trials = ctkb_all_trials.groupby(\"condition_concept_id\").count().to_dict()['nctid']\n",
    "dic_str = json.dumps(dic_num_trials)\n",
    "with open(\"test/num_trials_per_condition.txt\", 'w') as writer:\n",
    "    writer.write(dic_str)\n",
    "\n",
    "\"\"\"\n",
    "def jsonKeys2int(x):\n",
    "    if isinstance(x, dict):\n",
    "            return {int(k):v for k,v in x.items()}\n",
    "    return x\n",
    "\n",
    "with open(\"test/num_trials_per_condition.txt\", 'r') as reader:\n",
    "    dic=reader.read()\n",
    "dic_num_trials = json.loads(dic,object_hook=jsonKeys2int)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dnRivjQ1u0ro"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition: 313217\n",
      "inclusion:\n",
      "concept_id: 313217\n",
      "number of descendants: 13\n",
      "concept_name: Atrial fibrillation\n",
      "patient_counts: 61097\n",
      "1\n",
      "concept_id: 443237\n",
      "number of descendants: 8\n",
      "concept_name: Manic disorder, single episode\n",
      "patient_counts: 1835\n",
      "2\n",
      "concept_id: 4077953\n",
      "number of descendants: 1372\n",
      "concept_name: Therapy\n",
      "patient_counts: 50620\n",
      "3\n",
      "concept_id: 4322976\n",
      "4\n",
      "concept_id: 1351461\n",
      "number of descendants: 257\n",
      "concept_name: procainamide\n",
      "patient_counts: 2870\n",
      "5\n",
      "concept_id: 4154290\n",
      "number of descendants: 1\n",
      "concept_name: Paroxysmal atrial fibrillation\n",
      "patient_counts: 0\n",
      "6\n",
      "concept_id: 4266013\n",
      "number of descendants: 1\n",
      "concept_name: Medical contraindication\n",
      "patient_counts: 0\n",
      "7\n",
      "concept_id: 19008106\n",
      "number of descendants: 258\n",
      "concept_name: coumarin\n",
      "patient_counts: 0\n",
      "8\n",
      "concept_id: 381316\n",
      "number of descendants: 115\n",
      "concept_name: Cerebrovascular accident\n",
      "patient_counts: 0\n",
      "9\n",
      "concept_id: 4232697\n",
      "number of descendants: 2\n",
      "concept_name: Persistent atrial fibrillation\n",
      "patient_counts: 0\n",
      "10\n",
      "concept_id: 40760144\n",
      "number of descendants: 1\n",
      "concept_name: Data criteria Narrative\n",
      "patient_counts: 0\n",
      "11\n",
      "concept_id: 1310149\n",
      "number of descendants: 614\n",
      "concept_name: warfarin\n",
      "patient_counts: 25602\n",
      "12\n",
      "concept_id: 4353741\n",
      "number of descendants: 20\n",
      "concept_name: Cardioversion\n",
      "patient_counts: 1600\n",
      "13\n",
      "concept_id: 4091898\n",
      "number of descendants: 3\n",
      "concept_name: Sinus rhythm\n",
      "patient_counts: 0\n",
      "14\n",
      "concept_id: 4309345\n",
      "number of descendants: 1\n",
      "concept_name: Asymptomatic\n",
      "patient_counts: 0\n",
      "15\n",
      "concept_id: 44509524\n",
      "number of descendants: 0\n",
      "patient_counts: 0\n",
      "16\n",
      "concept_id: 42869419\n",
      "number of descendants: 1\n",
      "concept_name: ECG NEMSIS\n",
      "patient_counts: 0\n",
      "17\n",
      "concept_id: 4118403\n",
      "number of descendants: 6\n",
      "concept_name: Acardia\n",
      "patient_counts: 0\n",
      "18\n",
      "concept_id: 4078327\n",
      "number of descendants: 1\n",
      "concept_name: ENT symptoms\n",
      "patient_counts: 0\n",
      "19\n",
      "concept_id: 316866\n",
      "number of descendants: 138\n",
      "concept_name: Hypertensive disorder\n",
      "patient_counts: 19108\n",
      "20\n",
      "concept_id: 40241331\n",
      "number of descendants: 324\n",
      "concept_name: rivaroxaban\n",
      "patient_counts: 0\n",
      "21\n",
      "concept_id: 4038788\n",
      "number of descendants: 1\n",
      "concept_name: Persistence\n",
      "patient_counts: 0\n",
      "22\n",
      "concept_id: 314665\n",
      "number of descendants: 7\n",
      "concept_name: Atrial flutter\n",
      "patient_counts: 16273\n",
      "23\n",
      "concept_id: 4310996\n",
      "number of descendants: 11\n",
      "concept_name: Ischemic stroke\n",
      "patient_counts: 0\n",
      "24\n",
      "concept_id: 373503\n",
      "number of descendants: 14\n",
      "concept_name: Transient cerebral ischemia\n",
      "patient_counts: 14011\n",
      "25\n",
      "exclusion:\n",
      "concept_id: 315296\n",
      "number of descendants: 5\n",
      "concept_name: Preinfarction syndrome\n",
      "patient_counts: 103480\n",
      "1\n",
      "concept_id: 437312\n",
      "number of descendants: 1352\n",
      "concept_name: Bleeding\n",
      "patient_counts: 73101\n",
      "2\n",
      "concept_id: 3027172\n",
      "number of descendants: 1\n",
      "concept_name: Left ventricular Ejection fraction\n",
      "patient_counts: 116352\n",
      "3\n",
      "concept_id: 4101029\n",
      "number of descendants: 1\n",
      "concept_name: Left atrial dilatation\n",
      "patient_counts: 116352\n",
      "4\n",
      "concept_id: 316139\n",
      "number of descendants: 112\n",
      "concept_name: Heart failure\n",
      "patient_counts: 58477\n",
      "5\n",
      "concept_id: 4275564\n",
      "number of descendants: 1284\n",
      "concept_name: Operation on heart\n",
      "patient_counts: 103488\n",
      "6\n",
      "concept_id: 80205\n",
      "number of descendants: 6\n",
      "concept_name: Condition in fetus originating in the perinatal period\n",
      "patient_counts: 116350\n",
      "7\n",
      "concept_id: 4142479\n",
      "number of descendants: 69\n",
      "concept_name: Hyperthyroidism\n",
      "patient_counts: 95416\n",
      "8\n",
      "concept_id: 4299535\n",
      "number of descendants: 55\n",
      "concept_name: Pregnant\n",
      "patient_counts: 116140\n",
      "9\n",
      "concept_id: 4281749\n",
      "number of descendants: 781\n",
      "concept_name: Heart valve disorder\n",
      "patient_counts: 101956\n",
      "10\n",
      "concept_id: 318549\n",
      "number of descendants: 155\n",
      "concept_name: Cardiac septal defects\n",
      "patient_counts: 114144\n",
      "11\n",
      "concept_id: 4329847\n",
      "number of descendants: 91\n",
      "concept_name: Myocardial infarction\n",
      "patient_counts: 101860\n",
      "12\n",
      "concept_id: 1314425\n",
      "number of descendants: 1\n",
      "concept_name: Emergency surgery\n",
      "patient_counts: 116352\n",
      "13\n",
      "concept_id: 319835\n",
      "number of descendants: 33\n",
      "concept_name: Congestive heart failure\n",
      "patient_counts: 62558\n",
      "14\n",
      "concept_id: 3050013\n",
      "number of descendants: 1\n",
      "concept_name: Life expectancy [OASIS]\n",
      "patient_counts: 116352\n",
      "15\n",
      "\n",
      "time: 71.78099989891052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for condition_id in condition_ids:\n",
    "    print(\"condition:\", condition_id)\n",
    "    start = time.time()\n",
    "    #####step 1: get the person_id set for each TOP K inclusion and exclusion criteria\n",
    "    inclusions_original = inc_top[condition_id]\n",
    "    exclusions_original = exc_top[condition_id]\n",
    "\n",
    "    inclusions = []\n",
    "    exclusions = []\n",
    "\n",
    "    inclusions_no_patient = []\n",
    "    exclusions_no_patient = []\n",
    "    \n",
    "    dic_inc_concept_names = {}\n",
    "    dic_exc_concept_names = {}\n",
    "\n",
    "    #Create the person_id set for each criterion in the Top K inclusion criteria list\n",
    "    print(\"inclusion:\")\n",
    "    i = 1\n",
    "    criteria  = {}\n",
    "    for concept_id in inclusions_original:\n",
    "        print(\"concept_id:\",concept_id)\n",
    "        if concept_id not in general_criteria:\n",
    "            person_ids, concept_name = search_database_concept_set_descendant(concept_id=concept_id, inc=True)\n",
    "            #Get the person_id set related to a condition and its descendant concepts\n",
    "            print(\"patient_counts:\", len(person_ids))\n",
    "            if len(person_ids) != 0:\n",
    "                criteria[concept_id] = person_ids\n",
    "                inclusions.append(concept_id)\n",
    "                dic_inc_concept_names[concept_id] = concept_name\n",
    "            else:\n",
    "                inclusions_no_patient.append(concept_id)\n",
    "        print(i)\n",
    "        i += 1\n",
    "    \n",
    "    #Create the person_id set for each criterion in the Top K exclusion criteria list\n",
    "    print(\"exclusion:\")\n",
    "    i = 1\n",
    "    for concept_id in exclusions_original:\n",
    "        print(\"concept_id:\",concept_id)\n",
    "        if concept_id not in general_criteria:\n",
    "            person_ids, concept_name = search_database_concept_set_descendant(concept_id=concept_id, inc=False)\n",
    "            #Get the person_id set related to a condition and its descendant concepts\n",
    "            print(\"patient_counts:\", len(person_ids))\n",
    "            if len(person_ids) != TOTAL_NUMPATIENTS:\n",
    "                exclusions.append(concept_id)\n",
    "                criteria[concept_id] = person_ids\n",
    "                dic_exc_concept_names[concept_id] = concept_name\n",
    "\n",
    "            else:\n",
    "                exclusions_no_patient.append(concept_id)\n",
    "        print(i)\n",
    "        i += 1\n",
    "\n",
    "    #save_obj(criteria, \"test/patient_count_per_concept/condition_\"+str(condition_id))\n",
    "    save_obj(inclusions, \"test/inclusions_with_patient/condition_\"+str(condition_id))\n",
    "    save_obj(exclusions, \"test/exclusions_with_patient/condition_\"+str(condition_id))\n",
    "    save_obj(inclusions_no_patient, \"test/inclusions_without_patient/condition_\"+str(condition_id))\n",
    "    save_obj(exclusions_no_patient, \"test/exclusions_without_patient/condition_\"+str(condition_id))\n",
    "\n",
    "    \n",
    "    \n",
    "    ######step 2: Create table with pattern frequency for each condition\n",
    "    \n",
    "    #Join ctkb_all_trials table and ctkb_all_criteira_dropped table to \n",
    "    merged_df = ctkb_all_trials.merge(ctkb_all_criteria_dropped)\n",
    "    dic_condition = dict(tuple(merged_df.groupby(\"condition_concept_id\")))\n",
    "    df = dic_condition[condition_id]\n",
    "    \n",
    "    #change the exclusion criteria id from positive to negative, and flag exclusion criterion in Top K list as 2, \n",
    "    #inclusion criterion in Top K list as 1, and criterion not in the list as 0.\n",
    "    df = df.apply(flag_and_negate_exc, axis=1)\n",
    "    df = df.loc[df['flag']!=0]\n",
    "    df = df[['nctid','criteria_concept_id', 'Count']]\n",
    "    df = df.sort_values(['nctid', 'criteria_concept_id'])\n",
    "    \n",
    "    #spread the table\n",
    "    df = df.groupby(['nctid','criteria_concept_id'])['Count'].sum().unstack().fillna(0).astype(int)\n",
    "\n",
    "    #generate patterns\n",
    "    c_ids = np.array(df.columns)\n",
    "    df[\"pattern\"] = df.apply(lambda x: tuple(c_ids[np.where(x)]), axis=1)\n",
    "    #df.to_csv(\"test/condition_tables/condition_\"+str(condition_id)+\".csv\",\n",
    "    #            header=True, sep=\"\\t\")\n",
    "\n",
    "    #generate patern frequency for one condition\n",
    "    freq_table = pd.DataFrame({'pattern':df['pattern'].tolist()})\n",
    "    freq_table['frequency'] = 1\n",
    "        #print(freq_table)\n",
    "    new_table = freq_table.groupby(['pattern'])['frequency'].sum()\n",
    "    new_table = new_table.reset_index()\n",
    "    new_table['pattern_concept_names'] = new_table.apply(name_combination, axis=1)\n",
    "    new_table.to_csv(\"test/frequency_tables/frequency_condition_\"+str(condition_id)+\".csv\",\n",
    "            header=True, sep=\"\\t\")\n",
    "\n",
    "    del df, freq_table, new_table, merged_df\n",
    "    gc.collect()\n",
    "    df=pd.DataFrame()\n",
    "    freq_table=pd.DataFrame()\n",
    "    new_table=pd.DataFrame()\n",
    "    merged_df = pd.DataFrame()\n",
    "    dic_condition.clear()  \n",
    "\n",
    "\n",
    "\n",
    "    #####step 3: find all the sub-patterns for each patterns, recaluate the pattern frequency, and generate sorted trial_frequency table\n",
    "    num_trials = dic_num_trials[condition_id]\n",
    "    freq_table = pd.read_csv(\"test/frequency_tables/frequency_condition_\"+str(condition_id)+\".csv\",sep=\"\\t\", index_col=0)\n",
    "    if freq_table.shape[0]!=0: #If the condition has related trials:\n",
    "        #traverse all combinations of criteria in a pattern to get all sub-patterns\n",
    "        temp = freq_table.apply(traverse_combinations, axis = 1).to_list()\n",
    "        frequency_condition_expand = pd.DataFrame(functools.reduce(operator.iconcat, temp, []), columns=['pattern', 'frequency'])\n",
    "        \n",
    "        #recalculate the frequency of a pattern \n",
    "        new_table = frequency_condition_expand.groupby(['pattern'])['frequency'].sum()\n",
    "        new_table = new_table.reset_index()\n",
    "        new_table['relative_frequency'] = round(new_table['frequency']/num_trials,4)\n",
    "        new_table['#_criteria'] = new_table.apply(lambda x: len(x['pattern']), axis=1)\n",
    "        new_table['pattern_concept_names'] = new_table.apply(name_combination, axis=1)\n",
    "        \n",
    "        #Sort the table by frequency in descending order\n",
    "        new_table = new_table.sort_values(['frequency'], ascending=[False], ignore_index=True)\n",
    "        new_table[['pattern', 'pattern_concept_names', 'frequency', 'relative_frequency', '#_criteria']].to_csv(\"test/frequency_tables_expand_sorted_by_f/frequency_condition_\"+str(condition_id)+\"_expand_f.csv\",\n",
    "                    header=True, sep=\"\\t\")\n",
    "        \n",
    "        #Sort the table by number of criteria in the pattern in ascending order and frequency in descending order\n",
    "        new_table['rank_in_group'] = new_table.groupby(by = [\"#_criteria\"]).cumcount()\n",
    "        new_table.index=pd.MultiIndex.from_arrays([list(new_table['#_criteria']),list(new_table['rank_in_group'])])\n",
    "        new_table = new_table.sort_index()                                \n",
    "        new_table = new_table[['pattern', 'pattern_concept_names', 'frequency', 'relative_frequency']]\n",
    "        new_table.to_csv(\"test/frequency_tables_expand_by_f_and_num_criteria/frequency_condition_\"+str(condition_id)+\"_expand_f_nc.csv\",\n",
    "                         header=True, sep=\"\\t\")\n",
    "    else:\n",
    "        #record the empty table\n",
    "        freq_table.to_csv(\"test/condition_with_no_trials_and_patterns/frequency_condition_\"+str(condition_id)+\".csv\",\n",
    "                          header=True, sep=\"\\t\")\n",
    "        \n",
    "    del freq_table, new_table, temp\n",
    "    gc.collect()\n",
    "    freq_table = pd.DataFrame()\n",
    "    temp = pd.DataFrame()\n",
    "    new_table = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #####step 4: Compute the patient counts for each pattern, and draw the directed graph\n",
    "    freq_table = pd.read_csv(\"test/frequency_tables_expand_by_f_and_num_criteria/frequency_condition_\"+str(condition_id)+\"_expand_f_nc.csv\",sep=\"\\t\", index_col=[0,1])\n",
    "\n",
    "    DG = nx.DiGraph()\n",
    "    DG.add_nodes_from([('root',{\"names\":condition_names[condition_id],\"color\": \"lightgray\", \"count\": TOTAL_NUMPATIENTS})])\n",
    "    for i in np.arange(UPPER_BOUND,0, -1):\n",
    "        if i==1: #If we want to include all patterns with one criterion,\n",
    "            freq_table_sub = freq_table.loc[[i]]\n",
    "        else: #If we want to include Top M patterns with one criterion,\n",
    "            freq_table_sub = freq_table.loc[(i,0):(i,TOP_M_PATTERNS-1)]\n",
    "        for index, row in freq_table_sub.iterrows():\n",
    "            pattern = eval(row['pattern'])\n",
    "\n",
    "          ## add sub-pattern with largest reduction rate\n",
    "            count = compute_count(pattern)\n",
    "            num_criteria = len(pattern)\n",
    "            DG.add_nodes_from([(pattern, \n",
    "                              {\"names\": row['pattern_concept_names'], \n",
    "                              \"frequency\": row['frequency'], \n",
    "                              \"relative frequency\": row['relative_frequency'],\n",
    "                              \"count\": count,\n",
    "                            \"color\": color_dic[num_criteria]})])\n",
    "            add_sub_pattern_with_largest_reduction_rate(pattern)\n",
    "            if i==1:\n",
    "                count = DG.nodes['root']['count']\n",
    "                count_pattern = DG.nodes[pattern]['count']\n",
    "                reduction_rate = round((count - count_pattern)/count,5)\n",
    "                reduction_rate = \"{:.2%}\".format(reduction_rate)\n",
    "                DG.add_weighted_edges_from([('root', pattern, reduction_rate)])\n",
    "\n",
    "      ##     \n",
    "      ## add all sub-patterns \n",
    "      #add_sub_pattern(pattern)\n",
    "    DG_json = json_graph.node_link_data(DG)\n",
    "    patterns = [str(node['id']) for node in DG_json['nodes']]\n",
    "    counts = [node['count'] for node in DG_json['nodes']]\n",
    "      \n",
    "    node_table = pd.DataFrame({'pattern': patterns, 'patient_counts': counts}) \n",
    "    freq_table = pd.read_csv(\"test/frequency_tables_expand_sorted_by_f/frequency_condition_\"+str(condition_id)+\"_expand_f.csv\",sep=\"\\t\", index_col=[0])\n",
    "    node_table = node_table.merge(freq_table, on=\"pattern\", how='left')\n",
    "    node_table = node_table.rename({\"frequency\":\"trial_frequency\", \"relative_frequency\": \"trial_relative_frequency\"}, axis='columns')\n",
    "    node_table.to_csv(\"test/DG_results/top_\"+str(TOP_M_PATTERNS)+\"_nodes_condition_\"+str(condition_id)+\".csv\", index=False,sep=\"\\t\")    \n",
    "    \n",
    "    edge_table = pd.DataFrame.from_dict(DG_json['links'])\n",
    "    edge_table.columns = ['reduction_rate','source_pattern', 'target_pattern']\n",
    "    edge_table = edge_table[['source_pattern', 'target_pattern', 'reduction_rate']]\n",
    "    edge_table.to_csv(\"test/DG_results/top_\"+str(TOP_M_PATTERNS)+\"_edges_condition_\"+str(condition_id)+\".csv\", index=False,sep=\"\\t\")\n",
    "    \n",
    "    save_obj(DG, \"test/DG_for_colab/DG_top_\"+str(TOP_M_PATTERNS)+\"_condition_\"+str(condition_id))\n",
    "    \n",
    "    del freq_table, node_table, edge_table\n",
    "    gc.collect()\n",
    "    freq_table = pd.DataFrame()\n",
    "    node_table = pd.DataFrame()\n",
    "    edge_table = pd.DataFrame()\n",
    "    criteria.clear()\n",
    "    DG.clear()\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"\")\n",
    "    print(\"time:\", end-start)\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "NisLowoZu6GQ"
   },
   "outputs": [],
   "source": [
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWNkoAnDu9xQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUfVG_QFvAP5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def pattern_sql_formulate(pattern):\n",
    "    This function is to generate sql query to compute the patient count for each pattern\n",
    "    if pattern ==\"root\": \n",
    "        sql = \"SELECT COUNT(DISTINCT person_id) FROM person;\"\n",
    "        return sql\n",
    "    i = 1\n",
    "    sql = \"SELECT COUNT(*) FROM (\"\n",
    "    for concept_id in pattern:\n",
    "        if concept_id > 0 :\n",
    "            sql +=\"(\"\n",
    "            s,_ = concept_set_sql_formulate(concept_id, True)\n",
    "            sql += s\n",
    "            sql +=\") INTERSECT \"\n",
    "        else:\n",
    "            sql +=\"(\"\n",
    "            s,_ = concept_set_sql_formulate(concept_id, True)\n",
    "            sql += s\n",
    "            sql +=\") INTERSECT \"\n",
    "        i += 1\n",
    "      \n",
    "    sql = sql[:-10]\n",
    "    sql +=\") AS A;\"\n",
    "    return sql\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Condition_numPatient_part1_run_locally.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
